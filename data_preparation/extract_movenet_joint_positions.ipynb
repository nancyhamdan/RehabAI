{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSHsLQmHePYz",
        "outputId": "a4ffc3a5-d2fc-40cf-e5e2-7659cc589b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am969zlmxE3S",
        "outputId": "c43de0b6-3cd1-41fa-c2af-b5dbcaa6ec15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "GVWOiTWwshrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvrN0iQiOxhR"
      },
      "source": [
        "## Load Model from TF hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeGHgANcT7a1"
      },
      "outputs": [],
      "source": [
        "model_name = \"movenet_thunder\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
        "\n",
        "if \"tflite\" in model_name:\n",
        "  if \"movenet_lightning_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  elif \"movenet_lightning_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    # TF Lite format expects tensor type of uint8.\n",
        "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
        "    # Invoke inference.\n",
        "    interpreter.invoke()\n",
        "    # Get the model prediction.\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return keypoints_with_scores\n",
        "\n",
        "else:\n",
        "  if \"movenet_lightning\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    model = module.signatures['serving_default']\n",
        "\n",
        "    # SavedModel format expects tensor type of int32.\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    # Run model inference.\n",
        "    outputs = model(input_image)\n",
        "    # Output is a [1, 1, 17, 3] tensor.\n",
        "    keypoints_with_scores = outputs['output_0'].numpy()\n",
        "    return keypoints_with_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Joint Positions from Videos"
      ],
      "metadata": {
        "id": "3yi3tMCKwvd-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SYFdK-JHYhrv"
      },
      "outputs": [],
      "source": [
        "#@title Cropping Algorithm\n",
        "\n",
        "# Confidence score to determine whether a keypoint prediction is reliable.\n",
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "def init_crop_region(image_height, image_width):\n",
        "  \"\"\"Defines the default crop region.\n",
        "\n",
        "  The function provides the initial crop region (pads the full image from both\n",
        "  sides to make it a square image) when the algorithm cannot reliably determine\n",
        "  the crop region from the previous frame.\n",
        "  \"\"\"\n",
        "  if image_width > image_height:\n",
        "    box_height = image_width / image_height\n",
        "    box_width = 1.0\n",
        "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "    x_min = 0.0\n",
        "  else:\n",
        "    box_height = 1.0\n",
        "    box_width = image_height / image_width\n",
        "    y_min = 0.0\n",
        "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "  return {\n",
        "    'y_min': y_min,\n",
        "    'x_min': x_min,\n",
        "    'y_max': y_min + box_height,\n",
        "    'x_max': x_min + box_width,\n",
        "    'height': box_height,\n",
        "    'width': box_width\n",
        "  }\n",
        "\n",
        "def torso_visible(keypoints):\n",
        "  \"\"\"Checks whether there are enough torso keypoints.\n",
        "\n",
        "  This function checks whether the model is confident at predicting one of the\n",
        "  shoulders/hips which is required to determine a good crop region.\n",
        "  \"\"\"\n",
        "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE) and\n",
        "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE))\n",
        "\n",
        "def determine_torso_and_body_range(\n",
        "    keypoints, target_keypoints, center_y, center_x):\n",
        "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
        "\n",
        "  The function returns the maximum distances from the two sets of keypoints:\n",
        "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
        "  used to determine the crop size. See determineCropRegion for more detail.\n",
        "  \"\"\"\n",
        "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "  max_torso_yrange = 0.0\n",
        "  max_torso_xrange = 0.0\n",
        "  for joint in torso_joints:\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "    if dist_y > max_torso_yrange:\n",
        "      max_torso_yrange = dist_y\n",
        "    if dist_x > max_torso_xrange:\n",
        "      max_torso_xrange = dist_x\n",
        "\n",
        "  max_body_yrange = 0.0\n",
        "  max_body_xrange = 0.0\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "      continue\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
        "    if dist_y > max_body_yrange:\n",
        "      max_body_yrange = dist_y\n",
        "\n",
        "    if dist_x > max_body_xrange:\n",
        "      max_body_xrange = dist_x\n",
        "\n",
        "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
        "\n",
        "def determine_crop_region(\n",
        "      keypoints, image_height,\n",
        "      image_width):\n",
        "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
        "\n",
        "  The algorithm uses the detected joints from the previous frame to estimate\n",
        "  the square region that encloses the full body of the target person and\n",
        "  centers at the midpoint of two hip joints. The crop size is determined by\n",
        "  the distances between each joints and the center point.\n",
        "  When the model is not confident with the four torso joint predictions, the\n",
        "  function returns a default crop which is the full image padded to square.\n",
        "  \"\"\"\n",
        "  target_keypoints = {}\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    target_keypoints[joint] = [\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "    ]\n",
        "\n",
        "  if torso_visible(keypoints):\n",
        "    center_y = (target_keypoints['left_hip'][0] +\n",
        "                target_keypoints['right_hip'][0]) / 2;\n",
        "    center_x = (target_keypoints['left_hip'][1] +\n",
        "                target_keypoints['right_hip'][1]) / 2;\n",
        "\n",
        "    (max_torso_yrange, max_torso_xrange,\n",
        "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "          keypoints, target_keypoints, center_y, center_x)\n",
        "\n",
        "    crop_length_half = np.amax(\n",
        "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "    tmp = np.array(\n",
        "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "    crop_length_half = np.amin(\n",
        "        [crop_length_half, np.amax(tmp)]);\n",
        "\n",
        "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
        "\n",
        "    if crop_length_half > max(image_width, image_height) / 2:\n",
        "      return init_crop_region(image_height, image_width)\n",
        "    else:\n",
        "      crop_length = crop_length_half * 2;\n",
        "      return {\n",
        "        'y_min': crop_corner[0] / image_height,\n",
        "        'x_min': crop_corner[1] / image_width,\n",
        "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "        'height': (crop_corner[0] + crop_length) / image_height -\n",
        "            crop_corner[0] / image_height,\n",
        "        'width': (crop_corner[1] + crop_length) / image_width -\n",
        "            crop_corner[1] / image_width\n",
        "      }\n",
        "  else:\n",
        "    return init_crop_region(image_height, image_width)\n",
        "\n",
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
        "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
        "          crop_region['y_max'], crop_region['x_max']]]\n",
        "  output_image = tf.image.crop_and_resize(\n",
        "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "  return output_image\n",
        "\n",
        "def run_inference(movenet, image, crop_region, crop_size):\n",
        "  \"\"\"Runs model inferece on the cropped region.\n",
        "\n",
        "  The function runs the model inference on the cropped region and updates the\n",
        "  model output to the original image coordinate system.\n",
        "  \"\"\"\n",
        "  image_height, image_width, _ = image.shape\n",
        "  input_image = crop_and_resize(\n",
        "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "  # Run model inference.\n",
        "  keypoints_with_scores = movenet(input_image)\n",
        "  # Update the coordinates.\n",
        "  for idx in range(17):\n",
        "    keypoints_with_scores[0, 0, idx, 0] = (\n",
        "        crop_region['y_min'] * image_height +\n",
        "        crop_region['height'] * image_height *\n",
        "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "    keypoints_with_scores[0, 0, idx, 1] = (\n",
        "        crop_region['x_min'] * image_width +\n",
        "        crop_region['width'] * image_width *\n",
        "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "  return keypoints_with_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "  }"
      ],
      "metadata": {
        "id": "Gn00vuEAo_5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataframe_cols():\n",
        "  df_cols = []\n",
        "  for keypoint_name in KEYPOINT_DICT:\n",
        "    df_cols.append(f\"{keypoint_name}_y\")\n",
        "    df_cols.append(f\"{keypoint_name}_x\")\n",
        "    df_cols.append(f\"{keypoint_name}_confidence\")\n",
        "  return df_cols"
      ],
      "metadata": {
        "id": "0OHJknfzuJmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_frame_record(keypoints):\n",
        "  record = []\n",
        "  for keypoint in keypoints[0][0]:\n",
        "    record.append(keypoint[0])\n",
        "    record.append(keypoint[1])\n",
        "    record.append(keypoint[2])\n",
        "  return record"
      ],
      "metadata": {
        "id": "40YrHvjegyNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(exercise_rgb_dir, exercise_label_path, input_video_name, output_video_name):\n",
        "  # Open the video file using OpenCV\n",
        "  cap = cv2.VideoCapture(f\"{exercise_rgb_dir}/rgb/{input_video_name}\")\n",
        "\n",
        "  # Get video properties\n",
        "  frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "  frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "  # Initialize the cropping region\n",
        "  crop_region = init_crop_region(frame_height, frame_width)\n",
        "\n",
        "  # Store keypoints and scores for each frame\n",
        "  video_records = []\n",
        "  while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    # Run inference on the frame to detect keypoints\n",
        "    keypoints_with_scores = run_inference(\n",
        "        movenet, frame,\n",
        "        crop_region,\n",
        "        crop_size=[input_size, input_size])\n",
        "\n",
        "    # Adjust the cropping region based on detected keypoints\n",
        "    crop_region = determine_crop_region(keypoints_with_scores, frame_height, frame_width)\n",
        "\n",
        "    # Extract data for the frame record\n",
        "    frame_record = get_video_frame_record(keypoints_with_scores)\n",
        "    video_records.append(frame_record)\n",
        "\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()\n",
        "\n",
        "  # Create a DataFrame from the video records\n",
        "  video_df_cols = get_dataframe_cols()\n",
        "  video_df = pd.DataFrame(data=video_records, columns=video_df_cols)\n",
        "  video_df.to_csv(f\"{exercise_rgb_dir}/{output_video_name}.csv\", index=False)\n",
        "\n",
        "  # Copy the exercise label file to the output directory\n",
        "  cmd = f\"cp {exercise_label_path} {exercise_rgb_dir}\"\n",
        "  subprocess.run(cmd, shell=True)"
      ],
      "metadata": {
        "id": "z37ZnjItuJCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises processed:\n",
        "Exercise 1 <br>\n",
        "Processed 78 participants.<br>\n",
        "CPU times: user 2h 8min 10s, sys: 2min 20s, total: 2h 10min 31s<br>\n",
        "Wall time: 1h 35min 4s<br>\n",
        "<br>\n",
        "Exercise 4<br>\n",
        "Processed 78 participants.<br>\n",
        "CPU times: user 2h 16min 2s, sys: 2min 25s, total: 2h 18min 28s<br>\n",
        "Wall time: 1h 39min 27s<br>\n",
        "<br>\n",
        "Exercise 2<br>\n",
        "Processed 78 participants.<br>\n",
        "CPU times: user 2h 22min 38s, sys: 2min 43s, total: 2h 25min 22s<br>\n",
        "Wall time: 1h 44min 52s<br>\n",
        "<br>\n",
        "Exercise 3<br>\n",
        "Processed 78 participants.<br>\n",
        "CPU times: user 2h 2min 41s, sys: 3min 6s, total: 2h 5min 48s<br>\n",
        "Wall time: 1h 29min 9s<br>\n",
        "<br>\n",
        "Exercise 5<br>\n",
        "Processed 78 participants.<br>\n",
        "CPU times: user 1h 35min 5s, sys: 1min 53s, total: 1h 36min 59s<br>\n",
        "Wall time: 1h 9min 42s<br>"
      ],
      "metadata": {
        "id": "ZgaQnGlA_IV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KIMORE_RGB = \"/content/gdrive/MyDrive/PSUT/Graduation-Project2/rehab-ai-data/KiMoRe_rgb_movenet\"\n",
        "KIMORE = \"/content/gdrive/MyDrive/PSUT/Graduation-Project2/rehab-ai-data/KiMoRe\"\n",
        "EXERCISE = \"Es5\""
      ],
      "metadata": {
        "id": "nF-0Affi7XSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "print(f\"Processing Exercise {EXERCISE} ...\")\n",
        "unprocessed_videos = []\n",
        "processed_counter = 0\n",
        "\n",
        "# Iterate through the data directory structure\n",
        "for group in os.listdir(KIMORE_RGB):\n",
        "  for sub_group in os.listdir(f\"{KIMORE_RGB}/{group}\"):\n",
        "    sub_group_path = f\"{KIMORE_RGB}/{group}/{sub_group}\"\n",
        "    for participant in os.listdir(sub_group_path):\n",
        "      processed_counter += 1\n",
        "\n",
        "      # Construct paths for participant data\n",
        "      print(f\"\\t{participant}\")\n",
        "      participant_dir = f\"{sub_group_path}/{participant}\"\n",
        "      exercise_rgb_dir = f\"{participant_dir}/{EXERCISE}\"\n",
        "      exercise_dir = f\"{KIMORE}/{group}/{sub_group}/{participant}/{EXERCISE}/Label/\"\n",
        "      exercise_label_path = None\n",
        "\n",
        "      # Find the label file for the exercise\n",
        "      for file in os.listdir(exercise_dir):\n",
        "        if file.startswith(\"ClinicalAssessment\"):\n",
        "          exercise_label_path = f\"{exercise_dir}/{file}\"\n",
        "\n",
        "      # Check if the exercise video exists\n",
        "      if not os.listdir(f\"{exercise_rgb_dir}/rgb\"):\n",
        "        print(f\"\\t\\t\\tNo video found for exercise {EXERCISE} of {participant}.\")\n",
        "        unprocessed_videos.append(exercise_rgb_dir)\n",
        "        continue\n",
        "\n",
        "      # Find the input video file\n",
        "      input_video_name = None\n",
        "      for file in os.listdir(f\"{exercise_rgb_dir}/rgb\"):\n",
        "        if file.endswith(\".mp4\"):\n",
        "          input_video_name = file\n",
        "          output_video_name = f\"{participant}_{EXERCISE}\"\n",
        "          process_video(exercise_rgb_dir, exercise_label_path, input_video_name, output_video_name)\n",
        "          print(f\"\\t\\t\\tFinished processing {output_video_name}.csv\")\n",
        "\n",
        "print(f\"Processed {processed_counter} participants.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SzoSNJL_qt2",
        "outputId": "bcbdba28-a880-4026-edcd-b625459a1357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Exercise Es5 ...\n",
            "\tP_ID11\n",
            "\t\t\tFinished processing P_ID11_Es5.csv\n",
            "\tP_ID16\n",
            "\t\t\tFinished processing P_ID16_Es5.csv\n",
            "\tP_ID10\n",
            "\t\t\tFinished processing P_ID10_Es5.csv\n",
            "\tP_ID4\n",
            "\t\t\tFinished processing P_ID4_Es5.csv\n",
            "\tP_ID3\n",
            "\t\t\tNo video found for exercise Es5 of P_ID3.\n",
            "\tP_ID2\n",
            "\t\t\tFinished processing P_ID2_Es5.csv\n",
            "\tP_ID15\n",
            "\t\t\tFinished processing P_ID15_Es5.csv\n",
            "\tP_ID12\n",
            "\t\t\tFinished processing P_ID12_Es5.csv\n",
            "\tP_ID6\n",
            "\t\t\tFinished processing P_ID6_Es5.csv\n",
            "\tP_ID7\n",
            "\t\t\tFinished processing P_ID7_Es5.csv\n",
            "\tP_ID14\n",
            "\t\t\tFinished processing P_ID14_Es5.csv\n",
            "\tP_ID9\n",
            "\t\t\tFinished processing P_ID9_Es5.csv\n",
            "\tP_ID5\n",
            "\t\t\tFinished processing P_ID5_Es5.csv\n",
            "\tP_ID13\n",
            "\t\t\tFinished processing P_ID13_Es5.csv\n",
            "\tP_ID8\n",
            "\t\t\tFinished processing P_ID8_Es5.csv\n",
            "\tP_ID1\n",
            "\t\t\tFinished processing P_ID1_Es5.csv\n",
            "\tS_ID2\n",
            "\t\t\tNo video found for exercise Es5 of S_ID2.\n",
            "\tS_ID5\n",
            "\t\t\tFinished processing S_ID5_Es5.csv\n",
            "\tS_ID3\n",
            "\t\t\tFinished processing S_ID3_Es5.csv\n",
            "\tS_ID4\n",
            "\t\t\tFinished processing S_ID4_Es5.csv\n",
            "\tS_ID8\n",
            "\t\t\tFinished processing S_ID8_Es5.csv\n",
            "\tS_ID10\n",
            "\t\t\tFinished processing S_ID10_Es5.csv\n",
            "\tS_ID1\n",
            "\t\t\tFinished processing S_ID1_Es5.csv\n",
            "\tS_ID6\n",
            "\t\t\tFinished processing S_ID6_Es5.csv\n",
            "\tS_ID7\n",
            "\t\t\tFinished processing S_ID7_Es5.csv\n",
            "\tS_ID9\n",
            "\t\t\tFinished processing S_ID9_Es5.csv\n",
            "\tB_ID5\n",
            "\t\t\tFinished processing B_ID5_Es5.csv\n",
            "\tB_ID1\n",
            "\t\t\tFinished processing B_ID1_Es5.csv\n",
            "\tB_ID6\n",
            "\t\t\tFinished processing B_ID6_Es5.csv\n",
            "\tB_ID3\n",
            "\t\t\tFinished processing B_ID3_Es5.csv\n",
            "\tB_ID2\n",
            "\t\t\tFinished processing B_ID2_Es5.csv\n",
            "\tB_ID4\n",
            "\t\t\tFinished processing B_ID4_Es5.csv\n",
            "\tB_ID8\n",
            "\t\t\tFinished processing B_ID8_Es5.csv\n",
            "\tB_ID7\n",
            "\t\t\tFinished processing B_ID7_Es5.csv\n",
            "\tNE_ID27\n",
            "\t\t\tFinished processing NE_ID27_Es5.csv\n",
            "\tNE_ID19\n",
            "\t\t\tFinished processing NE_ID19_Es5.csv\n",
            "\tNE_ID26\n",
            "\t\t\tFinished processing NE_ID26_Es5.csv\n",
            "\tNE_ID11\n",
            "\t\t\tNo video found for exercise Es5 of NE_ID11.\n",
            "\tNE_ID18\n",
            "\t\t\tFinished processing NE_ID18_Es5.csv\n",
            "\tNE_ID16\n",
            "\t\t\tFinished processing NE_ID16_Es5.csv\n",
            "\tNE_ID21\n",
            "\t\t\tFinished processing NE_ID21_Es5.csv\n",
            "\tNE_ID20\n",
            "\t\t\tFinished processing NE_ID20_Es5.csv\n",
            "\tNE_ID4\n",
            "\t\t\tFinished processing NE_ID4_Es5.csv\n",
            "\tNE_ID5\n",
            "\t\t\tFinished processing NE_ID5_Es5.csv\n",
            "\tNE_ID17\n",
            "\t\t\tFinished processing NE_ID17_Es5.csv\n",
            "\tNE_ID2\n",
            "\t\t\tFinished processing NE_ID2_Es5.csv\n",
            "\tNE_ID12\n",
            "\t\t\tFinished processing NE_ID12_Es5.csv\n",
            "\tNE_ID10\n",
            "\t\t\tFinished processing NE_ID10_Es5.csv\n",
            "\tNE_ID15\n",
            "\t\t\tFinished processing NE_ID15_Es5.csv\n",
            "\tNE_ID3\n",
            "\t\t\tFinished processing NE_ID3_Es5.csv\n",
            "\tNE_ID23\n",
            "\t\t\tFinished processing NE_ID23_Es5.csv\n",
            "\tNE_ID24\n",
            "\t\t\tFinished processing NE_ID24_Es5.csv\n",
            "\tNE_ID22\n",
            "\t\t\tFinished processing NE_ID22_Es5.csv\n",
            "\tNE_ID25\n",
            "\t\t\tFinished processing NE_ID25_Es5.csv\n",
            "\tNE_ID14\n",
            "\t\t\tFinished processing NE_ID14_Es5.csv\n",
            "\tNE_ID13\n",
            "\t\t\tFinished processing NE_ID13_Es5.csv\n",
            "\tNE_ID6\n",
            "\t\t\tFinished processing NE_ID6_Es5.csv\n",
            "\tNE_ID9\n",
            "\t\t\tFinished processing NE_ID9_Es5.csv\n",
            "\tNE_ID8\n",
            "\t\t\tFinished processing NE_ID8_Es5.csv\n",
            "\tNE_ID1\n",
            "\t\t\tFinished processing NE_ID1_Es5.csv\n",
            "\tNE_ID7\n",
            "\t\t\tFinished processing NE_ID7_Es5.csv\n",
            "\tE_ID10\n",
            "\t\t\tFinished processing E_ID10_Es5.csv\n",
            "\tE_ID16\n",
            "\t\t\tFinished processing E_ID16_Es5.csv\n",
            "\tE_ID11\n",
            "\t\t\tFinished processing E_ID11_Es5.csv\n",
            "\tE_ID17\n",
            "\t\t\tFinished processing E_ID17_Es5.csv\n",
            "\tE_ID5\n",
            "\t\t\tFinished processing E_ID5_Es5.csv\n",
            "\tE_ID13\n",
            "\t\t\tFinished processing E_ID13_Es5.csv\n",
            "\tE_ID12\n",
            "\t\t\tFinished processing E_ID12_Es5.csv\n",
            "\tE_ID2\n",
            "\t\t\tFinished processing E_ID2_Es5.csv\n",
            "\tE_ID14\n",
            "\t\t\tFinished processing E_ID14_Es5.csv\n",
            "\tE_ID8\n",
            "\t\t\tFinished processing E_ID8_Es5.csv\n",
            "\tE_ID15\n",
            "\t\t\tFinished processing E_ID15_Es5.csv\n",
            "\tE_ID3\n",
            "\t\t\tFinished processing E_ID3_Es5.csv\n",
            "\tE_ID4\n",
            "\t\t\tFinished processing E_ID4_Es5.csv\n",
            "\tE_ID6\n",
            "\t\t\tFinished processing E_ID6_Es5.csv\n",
            "\tE_ID1\n",
            "\t\t\tFinished processing E_ID1_Es5.csv\n",
            "\tE_ID7\n",
            "\t\t\tFinished processing E_ID7_Es5.csv\n",
            "\tE_ID9\n",
            "\t\t\tFinished processing E_ID9_Es5.csv\n",
            "Processed 78 participants.\n",
            "CPU times: user 1h 35min 5s, sys: 1min 53s, total: 1h 36min 59s\n",
            "Wall time: 1h 9min 42s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "th6M20S5Ijlo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
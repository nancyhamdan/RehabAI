{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spFnEq5UK2l2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!!pip install -q git+https://github.com/keras-team/keras-nlp.git --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DVh6OXTQ0HD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.regularizers import l2, l1\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKDvB83tgnP4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYckdv4NisB5"
      },
      "outputs": [],
      "source": [
        "EXERCISE = 'Es5'\n",
        "\n",
        "max_length_mapping = {\n",
        "        \"Es1\": 1515,\n",
        "        \"Es2\": 1668,\n",
        "        \"Es3\": 1518,\n",
        "        \"Es4\": 1988,\n",
        "        \"Es5\": 1022\n",
        "    }\n",
        "\n",
        "temporal_windows = { #[num_windows, window_size]\n",
        "        \"Es1\": [5,303],\n",
        "        \"Es2\": [3,556],\n",
        "        \"Es3\": [11,138],\n",
        "        \"Es4\": [4,497],\n",
        "        \"Es5\": [7,146],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNf-wMRaJBzM"
      },
      "source": [
        "# Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wnPa4Xhd-K-"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/rehab-ai-data/KiMoRe_final/KiMoRe_data_movenet.csv\", index_col=False).drop('Unnamed: 0', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1oz5eDK5W3W"
      },
      "outputs": [],
      "source": [
        "df = df[df['exercise']==EXERCISE]\n",
        "print(f'Max frames: {df[\"#frames\"].unique().max()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL-VqavjeO6L"
      },
      "outputs": [],
      "source": [
        "def get_dataframe_cols():\n",
        "  KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "  }\n",
        "  df_cols = []\n",
        "  for keypoint_name in KEYPOINT_DICT:\n",
        "    df_cols.append(f\"{keypoint_name}_y\")\n",
        "    df_cols.append(f\"{keypoint_name}_x\")\n",
        "    df_cols.append(f\"{keypoint_name}_confidence\")\n",
        "  return df_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFcjrZEEeUOe"
      },
      "outputs": [],
      "source": [
        "all_cols = get_dataframe_cols()\n",
        "face_cols = all_cols[:15]\n",
        "cols_drop = face_cols\n",
        "print(f\"Dropping {len(cols_drop)} columns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiZas7ht2Zch"
      },
      "outputs": [],
      "source": [
        "print(f\"Maximum video length: {max_length_mapping[EXERCISE]}\")\n",
        "\n",
        "def prepare_data(df, exercise_video_max_len, data_type):\n",
        "  data = []\n",
        "  labels = []\n",
        "  padding_masks = []\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    joint_positions_path = row['joint_positions']\n",
        "    if joint_positions_path is np.NAN:\n",
        "      continue\n",
        "    clinical_score = row['clinical_score']\n",
        "    video_length = row['#frames']\n",
        "\n",
        "    joint_positions_data = pd.read_csv(joint_positions_path)\n",
        "    joint_positions_data = joint_positions_data.drop(cols_drop, axis=1)\n",
        "    joint_positions_data = joint_positions_data.to_numpy()\n",
        "\n",
        "    padding_length = exercise_video_max_len - video_length\n",
        "    padding_mask = np.zeros((video_length + padding_length))\n",
        "    padding_mask[-padding_length:] = 1\n",
        "\n",
        "    joint_positions_data_padded = np.pad(joint_positions_data, ((0, padding_length), (0, 0)), mode='constant', constant_values=0)\n",
        "\n",
        "    data.append(joint_positions_data_padded)\n",
        "    labels.append(clinical_score)\n",
        "    padding_masks.append(padding_mask)\n",
        "\n",
        "  data = np.array(data)\n",
        "  labels = np.array(labels)\n",
        "  padding_masks = np.array(padding_masks)\n",
        "\n",
        "  data = np.nan_to_num(data)\n",
        "  labels = np.nan_to_num(labels)\n",
        "\n",
        "  print(f\"{data_type} Data Shape:\", data.shape)\n",
        "  print(f\"{data_type} Labels Shape:\", labels.shape)\n",
        "  print(f\"{data_type} Padding Masks Shape:\", padding_masks.shape)\n",
        "\n",
        "  return (data, padding_masks), labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q18qUHS4cqL"
      },
      "outputs": [],
      "source": [
        "(all_data, all_padding), all_labels = prepare_data(df, max_length_mapping[EXERCISE], \"All Data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OWozp_iJFBF"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7XvYuks25E6"
      },
      "outputs": [],
      "source": [
        "NUM_JOINTS = all_data[0].shape[1]\n",
        "NUM_WINDOWS = temporal_windows[EXERCISE][0]\n",
        "WINDOW_SIZE = temporal_windows[EXERCISE][1]\n",
        "NUM_HEADS = 4\n",
        "D_MODEL = 10\n",
        "DENSE_UNITS = 64\n",
        "LEARNING_RATE = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVxTX2ej7hkc"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(all_data[0].shape[0], all_data[0].shape[1]), name='orignal_data')\n",
        "\n",
        "masks = tf.keras.Input(shape=(all_padding.shape[1]), name='padding_masks')\n",
        "\n",
        "windows = tf.split(inputs, NUM_WINDOWS, axis=1)\n",
        "print(\"Windows:\")\n",
        "for window in windows:\n",
        "  print(window.shape)\n",
        "\n",
        "windows_masks = tf.split(masks, NUM_WINDOWS, axis=1)\n",
        "print(\"Windows Masks:\")\n",
        "for mask in windows_masks:\n",
        "  print(mask.shape)\n",
        "\n",
        "embedding_layer = tf.keras.layers.Dense(18, activation='relu')\n",
        "embedding_layer3 = tf.keras.layers.Dense(10, activation='relu')\n",
        "\n",
        "embeddings = []\n",
        "for window in windows:\n",
        "    embedding = embedding_layer(window)\n",
        "    embedding = embedding_layer3(embedding)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "print(\"Embeddings:\")\n",
        "for embd in embeddings:\n",
        "  print(embd.shape)\n",
        "\n",
        "positional_embedding_layer = tf.keras.layers.Embedding(input_dim=WINDOW_SIZE, output_dim=D_MODEL)\n",
        "positional_embeddings = []\n",
        "for i in range(NUM_WINDOWS):\n",
        "    positional_embedding = positional_embedding_layer(tf.range(WINDOW_SIZE))\n",
        "    positional_embeddings.append(positional_embedding)\n",
        "\n",
        "print(\"Positional Embeddings:\")\n",
        "for pos_embd in positional_embeddings:\n",
        "  print(pos_embd.shape)\n",
        "\n",
        "embeddings_all = [embedding + positional_embedding for embedding, positional_embedding in zip(embeddings, positional_embeddings)]\n",
        "print(\"All Embeddings:\")\n",
        "for embd in embeddings_all:\n",
        "  print(embd.shape)\n",
        "\n",
        "transformer_encoder_layer = keras_nlp.layers.TransformerEncoder(intermediate_dim=D_MODEL, num_heads=NUM_HEADS)\n",
        "encoded = [transformer_encoder_layer(window_embd, window_mask) for window_embd, window_mask in zip(embeddings, windows_masks)]\n",
        "print(\"Encodings:\")\n",
        "for enc in encoded:\n",
        "  print(enc.shape)\n",
        "\n",
        "concat_output = tf.concat(encoded, axis=1)\n",
        "print(f\"Concat: {concat_output.shape}\")\n",
        "\n",
        "flatten_output = tf.keras.layers.Flatten()(concat_output)\n",
        "print(f\"Flatten: {flatten_output.shape}\")\n",
        "\n",
        "dense_output = tf.keras.layers.Dense(4970, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(flatten_output)\n",
        "dense_output = tf.keras.layers.Dense(621, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(dense_output)\n",
        "dense_output = tf.keras.layers.Dense(77, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(dense_output)\n",
        "print(f\"Final Dense: {dense_output.shape}\")\n",
        "output = tf.keras.layers.Dense(1)(dense_output)\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs, masks],\n",
        "                       outputs=output,\n",
        "                       name='transformer_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T69KPkAq9CTm"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_activations=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWH_ecsvMrPJ"
      },
      "source": [
        "# Plot Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHJnkSYmCO85"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(train_preds, train_labels, test_preds, test_labels, fold):\n",
        "  # Plot the predictions\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.suptitle(f'{EXERCISE} - Fold {fold}',fontsize=20)\n",
        "  plt.subplot(2, 1, 1)\n",
        "  plt.plot(train_preds, 's', color='red', label='Prediction', linestyle='None', alpha=0.5, markersize=6)\n",
        "  plt.plot(train_labels, 'o', color='green', label='Clinical Score', alpha=0.4, markersize=6)\n",
        "  plt.title('Training Set', fontsize=18)\n",
        "  plt.xlabel('Sequence Number', fontsize=16)\n",
        "  plt.ylabel('Clinical Score Scale', fontsize=16)\n",
        "  plt.legend(loc=3, prop={'size': 14})\n",
        "\n",
        "  plt.subplot(2, 1, 2)\n",
        "  plt.plot(test_preds, 's', color='red', label='Prediction', linestyle='None', alpha=0.5, markersize=6)\n",
        "  plt.plot(test_labels, 'o', color='green', label='Clinical Score', alpha=0.4, markersize=6)\n",
        "  plt.title('Testing Set', fontsize=18)\n",
        "  plt.xlabel('Sequence Number', fontsize=16)\n",
        "  plt.ylabel('Clinical Score Scale', fontsize=16)\n",
        "  plt.legend(loc=3, prop={'size': 14})\n",
        "\n",
        "  plt.tight_layout()\n",
        "  fig_title = f'{EXERCISE}_fold{fold}_pred_plot'\n",
        "  plt.savefig(f'/content/drive/MyDrive/rehab-ai-data/saved_models_images/{fig_title}.png', dpi=300)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRkyExWt597E"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, ptype, fold=None):\n",
        "  type_history = history.history[ptype]\n",
        "\n",
        "  epochs = range(len(type_history))\n",
        "  plt.plot(epochs, type_history, label=f'Training {ptype.capitalize()}')\n",
        "\n",
        "  if fold:\n",
        "    type_history_val = history.history[f'val_{ptype}']\n",
        "    plt.plot(epochs, type_history_val, label=f'Validation {ptype.capitalize()}')\n",
        "    plt.title(f'Training and Validation {ptype.capitalize()}')\n",
        "    plt.suptitle(f'{EXERCISE} - Fold {fold}')\n",
        "  else:\n",
        "    plt.title(f'{EXERCISE} {ptype.capitalize()}')\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(f'{ptype.capitalize()}')\n",
        "  plt.legend()\n",
        "\n",
        "  fig_title = f'{EXERCISE}_{ptype}_plot'\n",
        "  if fold:\n",
        "    fig_title += f'_fold{fold}'\n",
        "  plt.savefig(f'/content/drive/MyDrive/rehab-ai-data/saved_models_images/{fig_title}.png', dpi=300)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SKVmZG7KXuM"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU-t5LGjK6QZ"
      },
      "outputs": [],
      "source": [
        "class PrintEpochs(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch in [1, 25, 50, 75, 100]:\n",
        "              values = \", \".join([f\"{key}: {value:.4f}\" for key, value in logs.items()])\n",
        "              print(f\"Epoch {epoch}: {values}\")\n",
        "\n",
        "def cross_validate(model, data, labels, padding_masks, k=5):\n",
        "  y_true, y_pred, histories = list(), list(), list()\n",
        "  i = 1\n",
        "  kfold = KFold(n_splits=k, random_state=0, shuffle=True)\n",
        "  print(f\"Cross Validating Model Using {k} Folds...\")\n",
        "  for train_idx, val_idx in kfold.split(data):\n",
        "    print(f\"---------------- Fold {i} ----------------\")\n",
        "    X_train, X_val = data[train_idx], data[val_idx]\n",
        "    padding_train, padding_val = padding_masks[train_idx], padding_masks[val_idx]\n",
        "    y_train, y_val = labels[train_idx], labels[val_idx]\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mse', metrics=['mae'])\n",
        "\n",
        "    history = model.fit([X_train, padding_train], y_train, epochs=100, validation_data=([X_val, padding_val], y_val), verbose=0, callbacks=[PrintEpochs()])\n",
        "\n",
        "    train_pred = model.predict([X_train, padding_train])\n",
        "    val_pred = model.predict([X_val, padding_val])\n",
        "\n",
        "    y_true.extend(y_val)\n",
        "    y_pred.extend(val_pred)\n",
        "    histories.append(history)\n",
        "\n",
        "    fold_mae = mean_absolute_error(y_val, val_pred)\n",
        "    print(f'- MAE of fold {i} = {fold_mae}')\n",
        "\n",
        "    plot_history(history, 'loss', i)\n",
        "    plot_history(history, 'mae', i)\n",
        "    plot_predictions(train_pred, y_train, val_pred, y_val, i)\n",
        "    i = i+1\n",
        "\n",
        "\n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "  print(f'OOF MAE = {mae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo0aWplIQ22v"
      },
      "outputs": [],
      "source": [
        "cross_validate(model, all_data, all_labels, all_padding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIvjd-B2kWCY"
      },
      "source": [
        "# Final Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlYkusHhtmKZ"
      },
      "outputs": [],
      "source": [
        "for i in range(len(model.weights)):\n",
        "    model.weights[i]._handle_name = str(i) + '__' + model.weights[i].name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er_gLdgsfpWt"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mse', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBF03UpfrhCF"
      },
      "outputs": [],
      "source": [
        "checkpoint_filepath = f'/content/drive/MyDrive/rehab-ai-data/saved_models_weights/ml_model_{EXERCISE}_Weights.hdf5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_mae',\n",
        "    save_best_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjLeyf_vL7Zl"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "history = model.fit([all_data, all_padding], all_labels, epochs=100,\n",
        "                     callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLSMwN2kC9L8"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijkj6_YC9FR3"
      },
      "source": [
        "# Model History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0-y50An75H7"
      },
      "outputs": [],
      "source": [
        "plot_history(history, 'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e4aEz2Y8P_t"
      },
      "outputs": [],
      "source": [
        "plot_history(history, 'mae')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujIXQojSmSri"
      },
      "source": [
        "# Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzricXhemVVH"
      },
      "outputs": [],
      "source": [
        "model.load_weights(f'/content/drive/MyDrive/rehab-ai-data/saved_models_weights/ml_model_{EXERCISE}_Weights.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfhLZFl1melq"
      },
      "outputs": [],
      "source": [
        "!mkdir -p saved_model\n",
        "model.save(f'/content/drive/MyDrive/rehab-ai-data/saved_models_weights/ml_model_{EXERCISE}.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIVERIqgxB2E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
